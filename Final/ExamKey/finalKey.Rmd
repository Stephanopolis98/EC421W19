---
title: "EC 421"
subtitle: "Final Solutions"
author: "<br>18 March 2019"
date: "<br><br>**Full Name** `<-`<br><br>**UO ID** `<-`<br><br><br>No phones, calculators, or outside materials."
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      ratio: '8.5:11'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear

```{R, setup, include = F}
library(pacman)
p_load(
  here,
  knitr, kableExtra,
  ggplot2, ggthemes, viridis, ggforce, extrafont, gridExtra,
  tidyverse, magrittr
)
# Define colors
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F,
  echo = F,
  dev = "svg"
)
# opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(crayon.enabled = F)
options(knitr.table.format = "html")
# Function for formatting p values
format_pvi <- function(pv) {
  return(ifelse(
    pv < 0.0001,
    "<0.0001",
    round(pv, 4) %>% format(scientific = F)
  ))
}
format_pv <- function(pvs) lapply(X = pvs, FUN = format_pvi) %>% unlist()
```


## True/False

**37.5 points**

**Note:** You do not need to explain to your answers **in this section**.

**01\.** **[T/.pink[F]]** (**2.5pts**) $\text{Births}_t = \beta_0 + \beta_1 \text{Income}_t + \beta_2 \text{Births}_{t-1} + u_t$ describes a static time series model.
<br>.move-right[.hi-pink[FALSE]]<br>

**02\.** **[T/F]** (**2.5pts**) In the model $\text{Births}_t = \beta_0 + \beta_1 \text{Income}_t + u_t$, the parameter $\beta_1$ gives the effect of income in **all** previous time periods on births in the current period.
<br>.move-right[.hi-pink[FALSE]]<br>

**03\.** **[T/F]** (**2.5pts**) From the estimated model
$$
\begin{align}
  \text{Births}_t = \hat{\beta}_0 + \hat{\beta}_1 \text{Income}_{t} + \hat{\beta}_2 \text{Income}_{t-1} + \hat{\beta}_3 \text{Income}_{t-2} + e_t
\end{align}
$$
we can estimate the *total* effect of income on births as $\hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_2 + \hat{\beta}_3$.
<br>.move-right[.hi-pink[FALSE]]<br>

**04\.** **[T/F]** (**2.5pts**) If the disturbances from two time periods $t$ and $s$ (with $t\neq s$) have non-zero covariance, _i.e._, $\mathop{\text{Cov}} \left( u_t,\,u_s \right)\neq 0$, then we say that the disturbance is *heteroskedastic*.
<br>.move-right[.hi-pink[FALSE]]<br>

**05\.** **[T/F]** (**2.5pts**) For dynamic models with lagged explanatory variables and autocorrelated disturbances, OLS we can trust OLS to be unbiased.
<br>.move-right[.hi-pink[TRUE]]<br>

**06\.** **[T/F]** (**2.5pts**) If $\mathop{\text{Var}} \left( u_t \right) = 1$ for $t$ in {1, …, 10} and $\mathop{\text{Var}} \left( u_t \right) = 3$ for {11, …, T}, then $u_t$ is heteroskedastic.
<br>.move-right[.hi-pink[TRUE]]<br>

**07\.** **[T/F]** (**2.5pts**) If $\mathop{\text{Var}} \left( u_t \right) = 1$ for $t$ in {1, …, 10} and $\mathop{\text{Var}} \left( u_t \right) = 3$ for {11, …, T}, then $u_t$ is nonstationary.
<br>.move-right[.hi-pink[TRUE]]<br>

**08\.** **[T/F]** (**2.5pts**) Random walks are stationary.
<br>.move-right[.hi-pink[FALSE]]<br>

**09\.** **[T/F]** (**2.5pts**) Selection bias refers to including observations with differing variances.
<br>.move-right[.hi-pink[FALSE]]<br>

---
class: clear

**10\.** **[T/F]** (**2.5pts**) *Prediction* focuses on estimating $\hat{\beta}$, while *casual inference* focuses on estimating $\hat{y}$.
<br>.move-right[.hi-pink[FALSE]]<br>

**11\.** **[T/F]** (**2.5pts**) In the Rubin causal model, $y_{1,i}$ refers to the outcome for individual $i$ when he/she does not receive treatment.
<br>.move-right[.hi-pink[FALSE]]<br>

**12\.** **[T/F]** (**2.5pts**) Randomized experiment help us avoid selection bias by (approximately) balancing $\mathop{Avg}\left( y_{1,i} | D_i = 1 \right)$ and $\mathop{Avg}\left( y_{1,i} | D_i = 0 \right)$.
<br>.move-right[.hi-pink[Too confusing—just grade for completion.]]<br>

**13\.** **[T/F]** (**2.5pts**) An instrumental variable is *exogenous* if it affects $y$ (the outcome) through the endogenous $x$ and through another explanatory variable $w$.
<br>.move-right[.hi-pink[FALSE]]<br>

**14\.** **[T/F]** (**2.5pts**) The relevance requirement of instrumental variables is untestable.
<br>.move-right[.hi-pink[FALSE]]<br>

**15\.** **[T/F]** (**2.5pts**) The exogeneity requirement of instrumental variables is untestable.
<br>.move-right[.hi-pink[TRUE]]<br>

## Short Answer

**62.5 points**

**Note:** You will typically need to explain/justify your answers in this section.

**16\.** (**2.5pts**) Write down an ADL(1,1) model for the effect of income on births.
<br>

.pink[

Must include current income, lagged income, lagged births, and a disturbance:
<br>
$\text{Births}_t = \beta_0 + \beta_1 \text{Income}_t + \beta_2 \text{Income}_{t-1} + \beta_3 \text{Births}_{t-1} + u_t$

]

<br><br>

**17\.** (**2.5pts**) Explain what negative autocorrelation in our disturbances means.<br>You can focus on an AR(1) process.

.pink[

"Negative autocorrelation in our disturbances" means that our disturbances are negatively correlated in time. *E.g.*, in an AR(1) process, a large *positive* disturbance is likely to be followed by a large *negative* disturbance.

]


---
class: clear

**18\.** (**3pts**) In the following dynamic time-series model, $u_t$ is first-order autocorrelated, _i.e._,
$$
\begin{align}
  \text{Health}_t &= \beta_0 + \beta_1 \text{Income}_t + \beta_2 \text{Health}_{t-1} + u_t &\text{model, }t \\
  \text{Health}_{t-1} &= \beta_0 + \beta_1 \text{Income}_{t-1} + \beta_2 \text{Health}_{t-2} + u_{t-1} &\text{model, }t-1 \\
  u_t &= \rho u_{t-1} + \varepsilon_t &\text{AR(1)}
\end{align}
$$
where $\varepsilon_t$ is "white noise"—independentaly and identically distributed with mean zero and variance $\sigma^2_\varepsilon$.

Explain why OLS will likely be biased for $\beta_2$—even if there are no omitted variables.

.pink[

This model is likely to be biased—even without an omitted variables—because dynamic models with lagged outcome variables ***and*** autocorrelation violate contemporaneous exogeneity. The disturbance $u_{t-1}$ affects $\text{Health}_{t-1}$ (the model for t-1), and $u_{t-1}$ also affects $u_{t-1}$. Thus $\text{Health}_{t-1}$ (a regressor in model $t$) and $u_{t}$ (the disturbance in model $t$) are likely correlated, which violates contemporaneous exogeneity (leading to potential bias).

]

<br>

**19\.** (**3pts**) Suppose we are concerned about first-degree autocorrelation for the model
$$
\begin{align}
  \text{Crime}_t = \beta_0 + \beta_1 \text{Police}_t + \beta_2 \text{Crime}_{t-1} + u_t
\end{align}
$$
To test whether $u_t$ has first-degree autocorrelation, we run a Breusch-Godfrey test and find an $F$ statistic of 3.98, which has a *p*-value of approximately 0.0488. State the conclusion of this test and interpret the results.

.pink[

With a *p*-value of 0.0488, we reject the null hypothesis (H.sub[o] is *no autocorrelation of degree 1*). We conclude that there is statistically significant evidence, at the 5-percent level, or first-order autocorrelation.

]

<br><br><br>

**20\.** (**2.5pts**) If we omit a variable that is autocorrelated, will our error term be autocorrelated?<br>Explain your answer.

.pink[

**Yes.** If we omit a variable, then that variable is in our error term. Thus, if a variable in the error term is autocorrelated, then the error term (containing that variable) is likely autocorrelated.

]

---
class: clear

**21\.** We have two individuals—Ali and Bob—and would like to use them to estimate the causal effect of college on income. Let $\text{Income}_{1,i}$ denote the income of individual $i$ if she went to college and $\text{Income}_{0,i}$ if she did not.
$$
\begin{align}
  \text{Income}_{1,\text{Ali}} &= 75,000 & \text{Income}_{0,\text{Ali}} &= 70,000 \\
  \text{Income}_{1,\text{Bob}} &= 65,000 & \text{Income}_{0,\text{Bob}} &= 50,000
\end{align}
$$

**i.** (**3pts**) Calculate the causal effect of education for Ali.

.pink[75,000 - 70,000 = 5,000]
<br><br>

**ii.** (**3pts**) Calculate the causal effect of education for Bob.

.pink[65,000 - 50,000 = 15,000]
<br><br>

**iii.** (**2.5pts**) Is the treatment effect constant? Briefly explain.

.pink[**No.** The two individuals have different treatment effects.]
<br><br><br><br>

**iv.** (**3pts**) Bob went to college; Ali did not. Estimate the effect of college using the difference estimator—the difference in the mean of the treatment group (college) and the mean of the control group (no college).

.pink[65,000 - 70,000 = -5,000]
<br><br><br><br>

**v.** (**4pts**) Do we have selection bias? Briefly explain your answer.

.pink[**Yes.** Our control group (Ali) does not provide a good *counterfactual* for the treatment group (Bob). Ali's non-college outcome (70,000) is much higher than Bob's non-college outcome (50,000).]

---
class: clear

**22\.** (**2.5pts**) What is the fundamental problem of causal inference?

.pink[The fundamental problem of causal inference is that we cannot simultaneously observe outcomes from an individual as treatment and control.

Another way to say this: We cannot observe y.sub[1,i] and y.sub[0,i] at the same time.]

**23\.**  (**4pts**) What problem does instrumental variables attempt to solve? How does it do it?

.pink[Instrumental variables attempts to solve violations of our requirement of exogeneity (can also say: an endogenous explanatory variable or omitted-variable bias).

To solve this problem, instrumental variables separates "good" (exogenous) variation in our endogenous variable from "bad" (endogenous) variation.]

**24\.** (**3pts**) What does it mean for an instrumental variable to be *relevant*?

.pink[An instrument is *relevant* if it *affects* our endogenous explanatory variable.]

<br>

**25\.**The probability limit of the instrumental variables estimator is
$$
\begin{align}
  \mathop{\text{plim}}\left( \hat{\beta}_1^{\text{IV}} \right) = \beta_1 + \dfrac{\mathop{\text{Cov}} \left( z,\,u \right)}{\mathop{\text{Cov}} \left( z,\,x \right)}
\end{align}
$$
where $z$ is our instrument, $u$ is the disturbance, and $x$ is our endogenous variable.

**i.** (**4pts**) How do the two requirements of a valid instrument enter into this equation?

.pink[The denominator relates to **relevance**: If our instrument $z$ is relevant for the endogenous variable $x$, then the denominator is not zero.

The numerator relates to **exogeneity**: If our instrument $z$ is truly exogenous, then it is uncorrelated with the disturbance $u$, which makes the numerator zero.]

<br>

**ii.** **T/F** (**2.5pts**) If we have a valid instrument, then $\hat{\beta}_1^{\text{IV}}$ is a consistent estimator for $\beta_1$.

.move-right[.hi-pink[TRUE]]

---
class: clear

**26\.** (**2.5pts**) Consider the random walk
$$
\begin{align}
  u_t = u_{t-1} + \varepsilon_t
\end{align}
$$
where $\varepsilon_t$ is stationary.

Take the difference between $u_t$ and its lag. Is this difference stationary?

.pink[

The difference: $u_{t} - u_{t-1} = u_{t-1} + \varepsilon_t - u_{t-1} = \varepsilon_t$, which is stationary (by assumption).

Therefore: yes, the difference is stationary.

]

<br>

**27\.** Imagine we've estimated the model
$$
\begin{align}
  \text{Income}_{i} = \beta_{0} + \beta_{1} \left( \text{Military service} \right)_{i} + u_{i}
\end{align}
$$
using two-stage least squares.

$\text{Income}_i$ gives the income of individual $i$, and $\text{Military service}_{i}$ is a binary indicator variable for whether individual $i$ served in the military.

Our instrument for military service is $\text{Male}_i$, an indicator variable for whether or not the individual is male.

The following two tables give the results of the first and second stage.

```{R, stage1}
col_names <- c("Term", "Coef. estimate", "Standard error", ".italic[t] stat.", ".italic[p] Value")
digits <- c(NA, 3, 3, 2, 5)
tibble(
  term = c("Intercept", "Is Male"),
  coef = c(0.014, 0.120),
  se = c(0.008, 0.046),
  t_stat = coef/se,
  p_value = pnorm(q = -abs(t_stat)) %>% multiply_by(2) %>% format_pvi()
) %>%
kable(
  col.names = col_names,
  escape = F,
  digits = digits,
  caption = ".bold[First-stage results] (Outcome variable: Military service)"
) %>%
# kable_styling(font_size = 20) %>%
row_spec(1:2, background = "white") %>%
row_spec(1:2, bold = F, color = "black")
```

.white[space]

```{R, stage2}
digits <- c(NA, NA, NA, 2, 5)
tibble(
  term = c("Intercept", "Military service"),
  coef = c("31,153.71", "1,104.85"),
  se = c("7,829.2", "329.10"),
  t_stat = c(31153.71/7829.2, 1104.85/329.10),
  p_value = pnorm(q = -abs(t_stat)) %>% multiply_by(2) %>% format_pvi()
) %>%
kable(
  col.names = col_names,
  escape = F,
  digits = digits,
  caption = ".bold[Second-stage results] (Outcome variable: Income)"
) %>%
# kable_styling(font_size = 20) %>%
row_spec(1:2, background = "white") %>%
row_spec(1:2, bold = F, color = "black")
```

**Questions on the next page...**

---
class: clear

**i.** (**3pts**) Write out the first-stage model that we estimated.

.pink[

$$
\begin{align}
  \text{Military Service}_i = \pi_0 + \pi_1 \text{Is Male}_i + v_i
\end{align}
$$

**Note:** They can use any letter for the coefficients and the disturbance. It's also fine if they plugged in the estimates.

]

<br>

**ii.** (**3pts**) Interpret the first-stage results. (What do they say about gender differences in military service?)

.pink[The first-stage estimates suggest that there is a statically significant difference in military service by gender. Specifically, the intercept tells us approximately 1.4 percent of women serve in the military. The coefficient on *Is Male* tells us *an additional* 12 percent of men serve in the military (thus, 13.4 percent of men serve in the military). The difference between men and women is statistically significant.]

**iii.** (**3pts**) Does it appear as though we have a *relevant* instrument? Explain your answer.

.pink[**Yes.** Our *t* statistic on *Is Male* in the first stage is strongly significant, which signals that our instrument is relevant for (strongly correlated with) our endogenous variable (military service).]

<br>

**iv.** (**3pts**) Does it seem likely that our instrument is *exogenous*? Explain your answer.

.pink[**No.** For *Is Male* to be exogenous we need (1) *Is Male* to be uncorrelated with any other omitted variables that affect income (unlikely to be true: there are gender differences across many omitted and important variables, *e.g.*, education) and (2) *Is Male* can only affect income through military service (also unlikely—*e.g.*, labor-market discrimination).

**Note:** They only need to give a good reason *why* the instrument is not exogenous (they don't need this full answer).]

**v.** (**3pts**) Assuming that we have a valid instrument, interpret the second-stage results.

.pink[Based upon our second-stage results, prior military service causes a statistically significant increase in income at the 5-percent level. This increase is approximately $1,104.85.]

---
class: clear

## Extra credit: Venn diagrams(!)

- Each circle illustrates a variable.
- Overlap refers to the (share of) correlatation between two variables.
- Dotted borders denote *omitted* variables.

```{R, venn_iv}
# Colors (order: x1, x2, x3, y, z)
venn_colors <- c(purple, red, "grey60", orange, red_pink)
# Line types (order: x1, x2, x3, y, z)
venn_lines <- c("solid", "dotted", "dotted", "solid", "solid")
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5,    1.5,   -1.0, -1.4),
  y  = c( 0.0,   -2.5,   -1.8,    2.0, -2.6),
  r  = c( 1.9,    1.5,    1.5,    1.3,  1.3),
  l  = c( "Y", "X[1]", "X[2]", "X[3]",  "Z"),
  xl = c( 0.0,    0.8,    1.6,   -1.0, -2.9),
  yl = c( 0.0,   -3.9,   -1.9,    2.2, -2.6)
)
# Venn
v1 <- ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 6, family = "Fira Sans Book", parse = T) +
annotate(
  x = -3.6, y = 3.4,
  geom = "text", label = "Figure A", size = 6, family = "Fira Sans SemiBold", fontface = "bold", hjust = 0
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

```{R, venn_iv_endog}
# Change locations of circles
v2 <- venn_df %>%
mutate(
  x = x +   c(0, 0, 0, 0, 0),
  xl = xl + c(0, 0, 0, 0, 0),
  y = y +   c(0, 0, 0, 0, 1),
  yl = yl + c(0, 0, 0, 0, 1)
) %>%
# Venn
ggplot(data = ., aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 6, family = "Fira Sans Book", parse = T) +
annotate(
  x = -3.6, y = 3.4,
  geom = "text", label = "Figure B", size = 6, family = "Fira Sans SemiBold", fontface = "bold", hjust = 0
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

```{R, venn_iv_irrelevant}
# Change locations of circles
v3 <- venn_df %>%
mutate(
  x = x +   c(0, 0, 0, 0,-1),
  xl = xl + c(0, 0, 0, 0,-1),
  y = y +   c(0, 0, 0, 0, 2.3),
  yl = yl + c(0, 0, 0, 0, 2.3)
) %>%
# Venn
ggplot(data = ., aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 6, family = "Fira Sans Book", parse = T) +
annotate(
  x = -3.6, y = 3.4,
  geom = "text", label = "Figure C", size = 6, family = "Fira Sans SemiBold", fontface = "bold", hjust = 0
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

```{R, venn_iv_endog2}
# Change locations of circles
v4 <- venn_df %>%
mutate(
  x = x +   c(0,    0,   0, 0,    2),
  xl = xl + c(0, -2.4, 0.8, 0,  4.6),
  y = y +   c(0,    0,   0, 0,    0),
  yl = yl + c(0,    0,   0, 0, -1.1)
) %>%
# Venn
ggplot(data = ., aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 6, family = "Fira Sans Book", parse = T) +
annotate(
  x = -3.6, y = 3.4,
  geom = "text", label = "Figure D", size = 6, family = "Fira Sans SemiBold", fontface = "bold", hjust = 0
) +
xlim(-5.5, 4.5) +
ylim(-4.2, 3.4) +
coord_equal()
```

```{R, plot_venns, fig.height = 8.5}
grid.arrange(v1, v2, v3, v4, ncol = 2)
```
**Note:** You do not need to explain to your answers **in this section**.

**EC.sub[1].**  (**2pts**) Label the area(s) in **Figure A** that make us concerned about omitted-variable bias.
<br>   .hi-pink[Label should denote the intersection between X.sub[1] and X.sub[2].]

**EC.sub[2].**  (**2pts**) In which figures is Z a valid instrument for X.sub[1]? .hi-pink[Only A]

**EC.sub[3].**  (**2pts**) In which figures is Z *relevant* for X.sub[1]? .hi-pink[A, B, and D]

**EC.sub[4].**  (**2pts**) In which figures is Z *exogenous* (with respect to X.sub[1])? .hi-pink[Only A]

**EC.sub[5].**  (**2pts**) On the back of this page, draw a Venn diagram that has two valid instruments for an endogenous variable. .pink[Two instruments (*e.g.*, Z.sub[1] and Z.sub[2]) intersect with X.sub[1]. They do not intersect any other variables. Z.sub[1] and Z.sub[2] can intersect with Y *only if* the intersection includes X.sub[1].]

```{R, generate pdfs, eval = T, include = F}
system("decktape remark finalKey.html finalKey.pdf --chrome-arg=--allow-file-access-from-files")
```
