---
title: "Autocorrelation"
subtitle: "EC 421, Set 8"
author: "Edward Rubin"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{R, setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(
  broom, here, tidyverse,
  latex2exp, ggplot2, ggthemes, viridis, extrafont, gridExtra,
  kableExtra,
  dplyr, magrittr, knitr, parallel
)
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
# Dark slate grey: #314f4f
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18, family = "STIXGeneral"),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  text = element_text(family = "Fira Sans Book"),
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```

# Prologue

---
name: schedule

# Schedule

## Last Time

Introduction to time series

## Today

Autocorrelation

## Upcoming

1. **Survey** due at 11:59pm on Wednesday
2. **Assignment** *extended* to noon on Saturday (02/09). .pink[Don't wait.]
3. **Midterm** next Tuesday (02/11)
---
layout: true
# Time series
## Review
---
class: inverse, middle
---
name: review

TODO

---
layout: false
class: inverse, middle

# Autocorrelation
---
layout: false
name: intro
# Autocorrelation
## What is it?

.hi[Autocorrelation] occurs when our disturbances are correlated over time, _i.e._, $\mathop{\text{Cov}} \left( u_t,\, u_s \right) \neq 0$ for $t\neq s$.

--

Another way to think about: If the *shock* from disturbance $t$ correlates with "nearby" shocks in $t-1$ and $t+1$.

--

*Note:* **Serial correlation** and **autocorrelation** are the same thing.

--

Why is autocorrelation prevalent in time-series analyses?
---
class: clear

.hi-slate[Positive autocorrelation]: Disturbances $\left( u_t \right)$ over time
```{R, positive auto u, echo = F, fig.height = 7.25}
# Number of observations
T <- 1e2
# Rho
rho <- 0.95
# Set seed and starting point
set.seed(1234)
start <- rnorm(1)
# Generate the data
ar_df <- tibble(
  t = 1:T,
  x = runif(T, min = 0, max = 1),
  e = rnorm(T, mean = 0, sd = 2),
  u = NA
)
for (x in 1:T) {
  if (x == 1) {
    ar_df$u[x] <- rho * start + ar_df$e[x]
  } else {
    ar_df$u[x] <- rho * ar_df$u[x-1] + ar_df$e[x]
  }
}
ar_df %<>% mutate(y = 1 + 3 * x + u)
# Plot disturbances over time
ggplot(data = ar_df,
  aes(t, u)
) +
geom_line(color = "grey85", size = 0.35) +
geom_point(color = red_pink, size = 2.25) +
ylab("u") +
xlab("t") +
# theme_pander(base_family = "Fira Sans Book", base_size = 20)
theme_axes_math
```
---
class: clear

.hi-slate[Positive autocorrelation]: Outcomes $\left( y_t \right)$ over time
```{R, positive auto y, echo = F, fig.height = 7.25}
# Plot outcomes over time
ggplot(data = ar_df,
  aes(t, y)
) +
geom_line(color = "grey85", size = 0.35) +
geom_point(color = red_pink, size = 2.25) +
ylab("y") +
xlab("t") +
# theme_pander(base_family = "Fira Sans Book", base_size = 20)
theme_axes_math
```
---
class: clear

.hi-slate[Negative autocorrelation]: Disturbances $\left( u_t \right)$ over time
```{R, negative auto u, echo = F, fig.height = 7.25}
# Number of observations
T <- 1e2
# Rho
rho <- -0.95
# Set seed and starting point
set.seed(1234)
start <- rnorm(1)
# Generate the data
ar_df <- tibble(
  t = 1:T,
  x = runif(T, min = 0, max = 1),
  e = rnorm(T, mean = 0, sd = 2),
  u = NA
)
for (x in 1:T) {
  if (x == 1) {
    ar_df$u[x] <- rho * start + ar_df$e[x]
  } else {
    ar_df$u[x] <- rho * ar_df$u[x-1] + ar_df$e[x]
  }
}
ar_df %<>% mutate(y = 1 + 3 * x + u)
# Plot disturbances over time
ggplot(data = ar_df,
  aes(t, u)
) +
geom_line(color = "grey85", size = 0.35) +
geom_point(color = red_pink, size = 2.25) +
ylab("u") +
xlab("t") +
# theme_pander(base_family = "Fira Sans Book", base_size = 20)
theme_axes_math
```
---
class: clear

.hi-slate[Negative autocorrelation]: Outcomes $\left( y_t \right)$ over time
```{R, negative auto y, echo = F, fig.height = 7.25}
# Plot outcomes over time
ggplot(data = ar_df,
  aes(t, y)
) +
geom_line(color = "grey85", size = 0.35) +
geom_point(color = red_pink, size = 2.25) +
ylab("y") +
xlab("t") +
# theme_pander(base_family = "Fira Sans Book", base_size = 20)
theme_axes_math
```
---
layout: true
# Autocorrelation
## In static time-series models
---
name: static_models

Let's start with a very common model: a static time-series model whose disturbances exhibit .hi[first-order autocorrelation], *a.k.a.* .pink[AR(1)]:

$$
\begin{align}
  \text{Births}_t &= \beta_0 + \beta_1 \text{Income}_t + u_t
\end{align}
$$
where
$$
\begin{align}
  \color{#e64173}{u_t} = \color{#e64173}{\rho \, u_{t-1}} + \varepsilon_t
\end{align}
$$
and the $\varepsilon_t$ are independently and identically distributed (*i.i.d.*).

--

.hi-purple[Second-order autocorrelation], or .purple[AR(2)], would be

$$
\begin{align}
  \color{#6A5ACD}{u_t} = \color{#6A5ACD}{\rho_1 u_{t-1}} + \color{#6A5ACD}{\rho_2 u_{t-2}} + \varepsilon_t
\end{align}
$$
---

An .turquoise[AR(p)] model/process has a disturbance structure of

$$
\begin{align}
  u_t = \sum_{j=1}^\color{#20B2AA}{p} \rho_j u_{t-j} + \varepsilon_t
\end{align}
$$

allowing the current disturbance to correlated with up to $\color{#20B2AA}{p}$ of its lags.

---
layout: false
name: ols_static
# Autocorrelation
## OLS

For **static models** or **dynamic models with lagged explanatory variables**, in the presence of autocorrelation

1. OLS provides .pink[**unbiased** estimates for the coefficients.]

1. OLS creates .pink[**biased** estimates for the standard errors.]

1. OLS is .pink[**inefficient**.]

*Recall:* Same implications as heteroskedasticity.

Autocorrelation get trickier with lagged dependent variables.
---
layout: true
# Autocorrelation
## OLS and lagged dependent variables
---
name: ols_adl

Consider a model with one lag of the dependent variable—ADL(1, 0)—model with AR(1) disturbances

$$
\begin{align}
  \text{Births}_t = \beta_0 + \beta_1 \text{Income}_t + \beta_2 \text{Births}_{t-1} + u_t
\end{align}
$$
where
$$
\begin{align}
  u_t = \rho u_{t-1} + \varepsilon_t
\end{align}
$$

--

**Problem:**
--
 Both $\text{Births}_{t-1}$ (a regressor in the model for time $t$) and $u_{t}$ (the disturbance for time $t$) depend upon $u_{t-1}$. *I.e.*, a regressor is correlated with its contemporaneous disturbance.

--

**Q:** Why is this a problem?
--
<br>
**A:** It violates .pink[contemporaneous exogeneity]
--
, *i.e.*, $\mathop{\text{Cov}} \left( x_t,\,u_t \right) = 0$.
---

To see this problem, first write out the model for $t$ and $t-1$:
$$
\begin{align}
  \text{Births}_t &= \beta_0 + \beta_1 \text{Income}_t + \beta_2 \text{Births}_{t-1} + u_t \\
  \text{Births}_{t-1} &= \beta_0 + \beta_1 \text{Income}_{t-1} + \beta_2 \text{Births}_{t-2} + u_{t-1}
\end{align}
$$
and now note that $u_t = \rho u_{t-1} + \varepsilon_t$. Substituting...
$$
\begin{align}
  \text{Births}_t &= \beta_0 + \beta_1 \text{Income}_t + \beta_2 \color{#6A5ACD}{\text{Births}_{t-1}} + \overbrace{\left( \rho \color{#e64173}{u_{t-1}} + \varepsilon_t \right)}^{u_t} \tag{1} \\
  \color{#6A5ACD}{\text{Births}_{t-1}} &= \beta_0 + \beta_1 \text{Income}_{t-1} + \beta_2 \text{Births}_{t-2} + \color{#e64173}{u_{t-1}} \tag{2}
\end{align}
$$

In $(1)$, we can see that $u_t$ depends upon (covaries with) $\color{#e64173}{u_{t-1}}$.
<br> In $(2)$, we can see that $\color{#6A5ACD}{\text{Births}_{t-1}}$, a regressor in $(1)$, also covaries with $u_{t-1}$.

--

∴ This model violates our contemporaneous exogeneity requirement.
---

*Implications:* For models with **lagged dependent variables** and **autocorrelated disturbances**

1. The models .pink[**violate contemporaneous exogeneity**.]

1. OLS is .pink[**biased and inconsistent** for the coefficients.]

---

*Intuition?* Why is OLS inconsistent and biased when we violate exogeneity?

Think back to omitted-variable bias...

$$
\begin{align}
  y_t &= \beta_0 + \beta_1 x_t + u_t
\end{align}
$$

When $\mathop{\text{Cov}} \left( x_t,\, u_t \right)\neq0$, we cannot separate the effect of $u_t$ on $y_t$ from the effect of $x_t$ on $y_t$. Thus, we get inconsistent estimates for $\beta_1$.
--
 Similarly,

$$
\begin{align}
  \text{Births}_t &= \beta_0 + \beta_1 \text{Income}_t + \beta_2 \text{Births}_{t-1} + \overbrace{\left( \rho u_{t-1} + \varepsilon_t \right)}^{u_t} \tag{1}
\end{align}
$$

we cannot separate the effects of $u_t$ on $\text{Births}_t$ from $\text{Births}_{t-1}$ on $\text{Births}_{t}$, because both $u_t$ and $\text{Births}_{t-1}$ depend upon $u_{t-1}$.
--
 $\color{#e64173}{\hat{\beta}_2}$ .pink[is **biased** (w/ OLS).]
---
layout: true
# Autocorrelation and bias
## Simulation
---
name: ar2_simulation

To see how this bias can look, let's run a simulation.

$$
\begin{align}
  y_t &= 1 + 10 x_t + 100 y_{t-1} + u_t \\
  u_t &= 0.9 u_{t-1} + \varepsilon_t
\end{align}
$$


To generate 100 disturbances from AR(1), with $\rho = 0.9$:
<br>
`arima.sim(model = list(ar = c(0.9)), n = 100)`

--

We are going to run 1,000 iterations with $T=100$.

--

**Q:**  Will this simulation tell us about *bias* or *consistency*?

--

**A:** Bias. We would need to let $T\rightarrow\infty$ to consider consistency.
---

Outline of our simulation:

.pseudocode-small[
1. Generate T=100 values of x

1. Generate T=100 values of u

1. Use x and u to calculate T=100 values of y

1. Regress y on x; record estimates

Repeat 1–4 10,000 times
]
---
exclude: true

```{R, sim_gen, echo = F, eval = F}
# Define parameters
set.seed(1234)
rho <- 0.9
b0 <- 1; b1 <- 2; b2 <- 0.5
t <- 100
# Function for one iteration of the simulation
sim_fun <- function(x, rho, b0, b1, b2, t) {
  # Start generating data (initialize y as 1)
  data_x <- tibble(
    e = rnorm(t),
    u = rnorm(1),
    x = rnorm(t),
    y = 1
  )
  # Calculate u and y, iteratively
  for (j in 2:t) {
    data_x$u[j] <- rho * data_x$u[j-1] + data_x$e[j]
    data_x$y[j] <- b0 + b1 * data_x$x[j] + b2 * data_x$y[j-1] + data_x$u[j]
  }
  # Regression
  lm(y ~ x + lag(y), data = data_x) %>% tidy()
}
# Run the simulation 1,000 times
sim_df <- mclapply(
  X = 1:1e4,
  FUN = sim_fun,
  mc.cores = 8,
  rho = rho, b0 = b0, b1 = b1, b2 = b2, t = t
) %>% bind_rows()
# Save
saveRDS(
  object = sim_df,
  file = "sim.rds"
)
```

---
layout: false
class: clear

.hi-slate[Distribution of OLS estimates,] $\hat{\beta}_2$ in $y_t = 1 + 2 x_t + 0.5 y_{t-1} + u_t$

```{R, sim_density_b2, echo = F}
ggplot(data = readRDS("sim.rds") %>% filter(term == "lag(y)"), aes(x = estimate)) +
  geom_density(color = NA, fill = red_pink, alpha = 0.95) +
  geom_vline(xintercept = b2, linetype = "dashed") +
  geom_hline(yintercept = 0) +
  theme_pander(base_family = "Fira Sans Book", base_size = 20) +
  labs(x = "Estimate", y = "Density")
```

---
layout: false
class: clear

.hi-slate[Distribution of OLS estimates,] $\hat{\beta}_1$ in $y_t = 1 + 2 x_t + 0.5 y_{t-1} + u_t$

```{R, sim_density_b1, echo = F}
ggplot(data = readRDS("sim.rds") %>% filter(term == "x"), aes(x = estimate)) +
  geom_density(color = NA, fill = "orange", alpha = 0.95) +
  geom_vline(xintercept = b1, linetype = "dashed") +
  geom_hline(yintercept = 0) +
  theme_pander(base_family = "Fira Sans Book", base_size = 20) +
  labs(x = "Estimate", y = "Density")
```
---

**TODO:**

- Fix simulation description
- Remove sim.arima function
- Review of time series

---
layout: false
# Table of contents

.pull-left[
### Admin

1. [Schedule](#schedule)
1. [Review: Time series](#review)
]

.pull-right[
### Autocorrelation

1. [Introduction](#intro)
1. [In static models](#static_models)
1. [OLS, unbiased](#ols_static)
1. [OLS, biased](#ols_adl)
1. [Simulation: Bias](#ar2_simulation)
]
