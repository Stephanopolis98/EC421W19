---
title: "Metrics Review, Part 2"
subtitle: "EC 421, Set 3"
author: "Edward Rubin"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, center, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel)
# Define pink color
red_pink <- "#e64173"
# Notes directory
dir_slides <- "~/Dropbox/UO/Teaching/EC421W19/LectureNotes/02Review/"
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 14),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_bw() + theme(
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 14),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  legend.position = "none"
)
```

# Prologue

---

# .mono[R] showcase

**TODO**

**TODO**

---

# Schedule

## Last Time

We reviewed the fundamentals of statistics and econometrics.

**Follow up 1:** Someone asked about differences in R for Windows *vs.* Mac. One difference:  the characters you use for navigating the directory (file paths) within your computer.
- **Windows:**
  - *Option 1:* `"C:\\MyName\\Folder1\\Folder2"`
  - *Option 2:* `"C:/MyName/Folder1/Folder2"`
- **OSX** and **Linux:**: `"/User/MyName/Folder1/Folder2"`

--

**Follow up 2:** Is this font size better?

$$ \text{Diet} = \mathop{f}(\text{tacos},\, \text{pho},\, \text{kale},\, \text{water}) $$


---

# Schedule

## Today

We review of more of the main/basic results in metrics.

## This week

Will post the **first assignment** as the end of the week.

You will have a week to complete it.

---
class: inverse, middle, center

# Multiple regression

---
layout: true
# Multiple regression

---

## More explanatory variables

We're moving from **simple linear regression** (one outcome variable and one explanatory variable)

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$

to the land of **multiple linear regression** (one outcome variable and multiple explanatory variables)

$$ y\_i = \beta\_0 + \beta\_1 x\_{1i} + \beta\_2 x\_{2i} + \cdots + \beta\_k x\_{ki} + u\_i $$

--

**Why?**
--
 Better explain the variation in $y$, improve predictions, avoid omitted-variable bias, ...

---

```{R, gen data, cache = T, include = F}
n <- 1e2
set.seed(1234)
gen_df <- tibble(
  x1 = runif(n = n, min = -3, max = 3),
  x2 = sample(x = c(F, T), size = n, replace = T),
  u  = rnorm(n = n, mean = 0, sd = 1),
  y  = -0.5 + x1 + x2 * 4 + u
)
mean_a <- filter(gen_df, x2 == F)$y %>% mean()
mean_b <- filter(gen_df, x2 == T)$y %>% mean()
gen_df %<>% mutate(y_dm = y - mean_a * (x2 == F) - mean_b * (x2 == T))
```

$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + u_i \quad$ $x_1$ is continuous $\quad x_2$ is categorical

```{R, mult reg plot 1, dev = "svg", echo = F, fig.height = 6.25}
ggplot(data = gen_df, aes(y = y, x = x1, color = x2, shape = x2)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
annotate("text", x = -0.075, y = 7.75, label = TeX("$y$"), size = 4) +
annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 4) +
geom_point(size = 3) +
ylim(c(-4.5, 8)) +
theme_empty +
scale_color_manual(
  expression(x[2]),
  values = c("darkslategrey", red_pink),
  labels = c("A", "B")
) +
scale_shape_manual(
  expression(x[2]),
  values = c(1, 19),
  labels = c("A", "B")
) +
theme(legend.position = "bottom")
```

---
count: false

The intercept and categorical variable $x_2$ control for the groups' means.

```{R, mult reg plot 2, dev = "svg", echo = F, fig.height = 6.25}
ggplot(data = gen_df, aes(y = y, x = x1, color = x2, shape = x2)) +
geom_hline(yintercept = mean_a, color = "darkslategrey", alpha = 0.5) +
geom_hline(yintercept = mean_b, color = red_pink, alpha = 0.5) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
annotate("text", x = -0.075, y = 7.75, label = TeX("$y$"), size = 4) +
annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 4) +
geom_point(size = 3) +
ylim(c(-4.5, 8)) +
theme_empty +
scale_color_manual(
  expression(x[2]),
  values = c("darkslategrey", red_pink),
  labels = c("A", "B")
) +
scale_shape_manual(
  expression(x[2]),
  values = c(1, 19),
  labels = c("A", "B")
) +
theme(legend.position = "bottom")
```

---
count: false

With groups' means removed:

```{R, mult reg plot 3, dev = "svg", echo = F, fig.height = 6.25}
ggplot(data = gen_df %>% mutate(y = y - 4 * x2), aes(y = y_dm, x = x1)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
annotate("text", x = -0.075, y = 7.75, label = TeX("$y$"), size = 4) +
annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 4) +
geom_point(size = 3, aes(color = x2, shape = x2)) +
ylim(c(-4.5, 8)) +
theme_empty +
scale_color_manual(
  expression(x[2]),
  values = c("darkslategrey", red_pink),
  labels = c("A", "B")
) +
scale_shape_manual(
  expression(x[2]),
  values = c(1, 19),
  labels = c("A", "B")
) +
theme(legend.position = "bottom")
```

---
count: false

$\hat{\beta}_1$ estimates the relationship between $y$ and $x_1$ after controlling for $x_2$.

```{R, mult reg plot 4, dev = "svg", echo = F, fig.height = 6.25}
ggplot(data = gen_df %>% mutate(y = y - 4 * x2), aes(y = y_dm, x = x1)) +
geom_smooth(method = lm, se = F, color = "orange") +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
annotate("text", x = -0.075, y = 7.75, label = TeX("$y$"), size = 4) +
annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 4) +
geom_point(size = 3, aes(color = x2, shape = x2)) +
ylim(c(-4.5, 8)) +
theme_empty +
scale_color_manual(
  expression(x[2]),
  values = c("darkslategrey", red_pink),
  labels = c("A", "B")
) +
scale_shape_manual(
  expression(x[2]),
  values = c(1, 19),
  labels = c("A", "B")
) +
theme(legend.position = "bottom")
```

---
count: false

Another way to think about it:

```{R, mult reg plot 5, dev = "svg", echo = F, fig.height = 6.25}
ggplot(data = gen_df, aes(y = y, x = x1, color = x2, shape = x2)) +
geom_smooth(method = lm, se = F) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
annotate("text", x = -0.075, y = 7.75, label = TeX("$y$"), size = 4) +
annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 4) +
geom_point(size = 3) +
ylim(c(-4.5, 8)) +
theme_empty +
scale_color_manual(
  expression(x[2]),
  values = c("darkslategrey", red_pink),
  labels = c("A", "B")
) +
scale_shape_manual(
  expression(x[2]),
  values = c(1, 19),
  labels = c("A", "B")
) +
theme(legend.position = "bottom")
```

---

## Model fit

Measures of *goodness of fit* try to analyze how well our model describes (*fits*) the data.

**Common measure:** $R^2$ [R-squared] (*a.k.a.* coefficient of determination)

$$ R^2 = \dfrac{\sum_i (\hat{y}_i - \overline{y})^2}{\sum_i \left( y_i - \overline{y} \right)^2} = 1 - \dfrac{\sum_i \left( y_i - \hat{y}_i \right)^2}{\sum_i \left( y_i - \overline{y} \right)^2} $$

Notice our old friend SSE: $\sum_i \left( y_i - \hat{y}_i \right)^2 = \sum_i e_i^2$.

--

$R^2$ literally tells us the share of the variance in $y$ our current models accounts for. Thus $0 \leq R^2 \leq 1$.

---

**The problem:** As we add variables to our model, $R^2$ *mechanically* increases.

--

To see this problem, we can simulate a dataset of 10,000 observations on $y$ and 1,000 random $x_k$ variables. **No relations between $y$ and the $x_k$!**

```{R, r2 data code, eval = F}
set.seed(1234)
y <- rnorm(1e4)
x <- matrix(data = rnorm(1e7), nrow = 1e4)
x %<>% cbind(matrix(data = 1, nrow = 1e4, ncol = 1), x)
r_df <- mclapply(X = 1:(1e3-1), mc.cores = 12, FUN = function(i) {
  tmp_reg <- lm(y ~ x[,1:(i+1)]) %>% summary()
  data.frame(
    k = i + 1,
    r2 = tmp_reg %$% r.squared,
    r2_adj = tmp_reg %$% adj.r.squared
  )
}) %>% bind_rows()
```

---

**The problem:** As we add variables to our model, $R^2$ *mechanically* increases.


```{R, r2 data, include = F, cache = T}
set.seed(1234)
y <- rnorm(1e4)
x <- matrix(data = rnorm(1e7), nrow = 1e4)
x %<>% cbind(matrix(data = 1, nrow = 1e4, ncol = 1), x)
r_df <- mclapply(X = 1:(1e3-1), mc.cores = 12, FUN = function(i) {
  tmp_reg <- lm(y ~ x[,1:(i+1)]) %>% summary()
  data.frame(
    k = i + 1,
    r2 = tmp_reg %$% r.squared,
    r2_adj = tmp_reg %$% adj.r.squared
  )
}) %>% bind_rows()
```

```{R, r2 plot, echo = F, dev = "svg", fig.height = 6.4}
ggplot(data = r_df, aes(x = k, y = r2)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(size = 2, alpha = 0.75, color = "darkslategrey") +
geom_line(aes(y = r2_adj), size = 0.2, alpha = 0, color = red_pink) +
ylab(TeX("R^2")) +
xlab("Number of explanatory variables (k)") +
theme_pander(base_size = 14)
```

---

**One solution:** Adjusted $R^2$

```{R, adjusted r2 plot, echo = F, dev = "svg", fig.height = 6.4}
ggplot(data = r_df, aes(x = k, y = r2)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(size = 2, alpha = 0.15, color = "darkslategrey") +
geom_line(aes(y = r2_adj), size = 2, alpha = 0.85, color = red_pink) +
ylab(TeX("R^2")) +
xlab("Number of explanatory variables (k)") +
theme_pander(base_size = 14)
```

---

**The problem:** As we add variables to our model, $R^2$ *mechanically* increases.

**One solution:** Control (penalize) for the number of variables, _e.g._, adjusted $R^2$:

$$ \overline{R}^2 = 1 - \dfrac{\sum_i \left( y_i - \hat{y}_i \right)^2/(n-k-1)}{\sum_i \left( y_i - \overline{y} \right)^2/(n-1)} $$

*Note:* Adjusted $R^2$ need not be between 0 and 1.

---

## Tradeoffs

There are tradeoffs to remember as we add/remove variables:

**Fewer variables**

- Generally explain less variation in $y$
- Provide simple interpretations and visualizations (*parsimonious*)
- May need to worry about omitted-variable bias

**More variables**

- More likely to find *spurious* relationships (statistically significant due to chance—does not reflect a true, population-level relationship)
- More difficult to interpret the model
- You may still miss important variabless—still omitted-variable bias

---
layout: true
# Omitted-variable bias
---
class: inverse, middle, center

---

We'll go deeper into this issue in a few weeks, but as a refresher:

**Omitted-variable bias** (OVB) arises when we omit a variable that

1. affects our outcome variable $y$

2. correlates with an explanatory variable $x_j$

As it's name suggests, this situation leads to bias in our estimate of $\beta_j$.

--

**Note:** OVB Is not exclusive to multiple linear regression, but it does require multiple variables affect $y$.

---
**Example**

Let's imagine a simple model for the amount individual $i$ gets paid

$$ \text{Pay}_i = \beta_0 + \beta_1 \text{School}_i + \beta_2 \text{Male}_i + u_i $$

where

- $\text{School}_i$ gives $i$'s years of schooling
- $\text{Male}_i$ denotes an indicator variable for whether individual $i$ is male.

thus

- $\beta_1$: the returns to an additional year of schooling
- $\beta_2$: the premium for being male (if $\beta_2 > 0$, then there is discrimination against women—receiving less pay based upon gender)

---
layout: true
# Omitted-variable bias
**Example, continued**

---

From our population model

$$ \text{Pay}_i = \beta_0 + \beta_1 \text{School}_i + \beta_2 \text{Male}_i + u_i $$

If a study focuses on the relationship between pay and schooling, _i.e._,

$$ \text{Pay}_i = \beta_0 + \beta_1 \text{School}_i + \left(\beta_2 \text{Male}_i + u_i\right) $$
$$ \text{Pay}_i = \beta_0 + \beta_1 \text{School}_i + \varepsilon_i $$

where $\varepsilon_i = \beta_2 \text{Male}_i + u_i$.

We used our exogeneity assumption to derive OLS' unbiasedness. But even if $\mathop{\boldsymbol{E}}\left[ u | X \right] = 0$, it is not true that $\mathop{\boldsymbol{E}}\left[ \varepsilon | X \right] = 0$ so long as $\beta_2 \neq 0$.

Specifically, $\mathop{\boldsymbol{E}}\left[ \varepsilon | \text{Male} = 1 \right] = \beta_2 + \mathop{\boldsymbol{E}}\left[ u | \text{Male} = 1 \right] \neq 0$.
--
 **Now OLS is biased.**

---

Let's try to see this result graphically.

```{R, gen ovb data, include = F, cache = T}
# Set seed
set.seed(12345)
# Sample size
n <- 1e3
# Parameters
beta0 <- 20; beta1 <- 0.5; beta2 <- 10
# Dataset
omit_df <- tibble(
  male = sample(x = c(F, T), size = n, replace = T),
  school = runif(n, 3, 9) - 3 * male,
  pay = beta0 + beta1 * school + beta2 * male + rnorm(n, sd = 7)
)
lm_bias <- lm(pay ~ school, data = omit_df)
bb0 <- lm_bias$coefficients[1] %>% round(1)
bb1 <- lm_bias$coefficients[2] %>% round(1)
lm_unbias <- lm(pay ~ school + male, data = omit_df)
bu0 <- lm_unbias$coefficients[1] %>% round(1)
bu1 <- lm_unbias$coefficients[2] %>% round(1)
bu2 <- lm_unbias$coefficients[3] %>% round(1)
```

The population model:

$$ \text{Pay}_i = `r beta0` + `r beta1` \times \text{School}_i + `r beta2` \times \text{Male}_i + u_i $$

Our regression model that suffers from omitted-variable bias:

$$ \text{Pay}_i = \hat{\beta}_0 + \hat{\beta}_1 \times \text{School}_i + e_i $$

Finally, imagine that women, on average, receive more schooling than men.

---
layout: true
# Omitted-variable bias
**Example, continued:** $\text{Pay}_i = `r beta0` + `r beta1` \times \text{School}_i + `r beta2` \times \text{Male}_i + u_i$

---

The relationship between pay and schooling.

```{R, plot ovb 1, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, color = "black", alpha = 0.4, shape = 16) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 14),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)
```
---
count: false
Biased regression estimate: $\widehat{\text{Pay}}_i = `r bb0` + `r bb1` \times \text{School}_i$

```{R, plot ovb 2, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, color = "black", alpha = 0.4, shape = 16) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_smooth(se = F, color = "orange", method = lm) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 14),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)
```

---
count: false
**Recalling the omitted variable:** <font color="#e64173">female</font> and <font color="#314f4f">male</font>

```{R, plot ovb 3, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = male, shape = male)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(stat = "smooth", color = "orange", method = lm, alpha = 0.2, size = 1) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 14),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("Female", "Male")) +
scale_shape_manual("", values = c(16, 1), labels = c("Female", "Male"))
```

---
count: false
**Recalling the omitted variable:** <font color="#e64173">female</font> and <font color="#314f4f">male</font>

```{R, plot ovb 4, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = male, shape = male)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(stat = "smooth", color = "orange", method = lm, alpha = 0.2, size = 1) +
geom_smooth(
  aes(color = male, shape = male),
  method = lm, se = F
) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 14),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("Female", "Male")) +
scale_shape_manual("", values = c(16, 1), labels = c("Female", "Male"))
```

---
count: false
Biased regression estimate: $\widehat{\text{Pay}}_i = `r bu0` + `r bu1` \times \text{School}_i + `r bu2` \times \text{Male}_i$

```{R, plot ovb 5, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = omit_df, aes(x = school, y = pay)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = male, shape = male)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_abline(
  intercept = lm_unbias$coefficients[1], slope = lm_unbias$coefficients[2],
  color = "orange", size = 1
) +
xlab("Schooling") +
ylab("Pay") +
theme_empty +
theme(
  axis.title = element_text(size = 14),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("Female", "Male")) +
scale_shape_manual("", values = c(16, 1), labels = c("Female", "Male"))
```

---
layout: false

# Omitted-variable bias

## Solutions

1. Don't omit variables

2. Instrumental variables and two-stage least squares<sup>†</sup>

**Warning:** There are situations in which neither solution is possible.

--

1. Proceed with caution (sometimes you can sign the bias).

2. Stop.

.footnote[
[†]: Coming soon to our lectures.
]

---
layout: true
# Interpreting coefficients

---

## Continuous variables

## Categorical variables

## Interactions

## Log-linear specification

## Log-log specification

## Log-linear with a binary variable

---
layout: true
# Additional topics

---

## Treatment effects and causality

## Transformations

## Outliers

## Missing data
