<!DOCTYPE html>
<html>
  <head>
    <title>Metrics Review</title>
    <meta charset="utf-8">
    <meta name="author" content="Edward Rubin" />
    <meta name="date" content="2019-01-09" />
    <link href="02_review_files/remark-css/default.css" rel="stylesheet" />
    <link href="02_review_files/remark-css/metropolis.css" rel="stylesheet" />
    <link href="02_review_files/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Metrics Review
## EC 421, Set 2
### Edward Rubin
### 09 January 2019

---

class: inverse, center, middle



# Prologue

---

# Last Time

## Motivation

In our last set of slides, we

1. discussed the **motivation** for studying econometrics (metrics)

1. **introduced .mono[R]**—why we use it, what it can do

1. **started reviewing** material from your previous classes

These notes continue the review, building the foundation for some new topics (soon).

--

## Follow Up

.mono[R] is [available at all academic workstations at UO](https://library.uoregon.edu/library-technology-services/public-info/a-software).

---
layout: false
class: inverse, center, middle
# Review

---
layout: true
# Population *vs.* sample

---

## Models and notation

We write our (simple) population model

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$

and our sample-based estimated regression model as

$$ y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i $$

An estimated regession model produces estimates for each observation:

$$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$

which gives us the _best-fit_ line through our dataset.

---
layout: true

# Population *vs.* sample

**Question:** Why are we so worked up about the distinction between *population* and *sample*?

---

--



.pull-left[

&lt;img src="02_review_files/figure-html/pop1-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Population**]

]

--

.pull-right[

&lt;img src="02_review_files/figure-html/scatter1-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Population relationship**]

$$ y_i = 2.53 + 0.57 x_i + u_i $$

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$


]

---

.pull-left[

&lt;img src="02_review_files/figure-html/sample1-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Sample 1:** 30 random individuals]

]

--

.pull-right[

&lt;img src="02_review_files/figure-html/sample1 scatter-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Population relationship**]

$$ y_i = 2.53 + 0.57 x_i + u_i $$

.center[**Sample relationship**]

$$ \hat{y}_i = 1.36 + 0.76 x_i $$


]

---
count: false

.pull-left[

&lt;img src="02_review_files/figure-html/sample2-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Sample 2:** 30 random individuals]

]

.pull-right[

&lt;img src="02_review_files/figure-html/sample2 scatter-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Population relationship**]

$$ y_i = 2.53 + 0.57 x_i + u_i $$

.center[**Sample relationship**]

$$ \hat{y}_i = 3.53 + 0.34 x_i $$


]
---
count:false

.pull-left[

&lt;img src="02_review_files/figure-html/sample3-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Sample 3:** 30 random individuals]

]

.pull-right[

&lt;img src="02_review_files/figure-html/sample3 scatter-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Population relationship**]

$$ y_i = 2.53 + 0.57 x_i + u_i $$

.center[**Sample relationship**]

$$ \hat{y}_i = 1.44 + 0.86 x_i $$

]

---
layout: false
class: inverse, center, middle

Let's repeat this **10,000 times**.

(This exercise is called a (Monte Carlo) simulation.)

---
layout: true
# Population *vs.* sample

---

&lt;img src="02_review_files/figure-html/simulation scatter-1.png" style="display: block; margin: auto;" /&gt;

---
layout: true
# Population *vs.* sample

**Question:** Why are we so worked up about the distinction between *population* and *sample*?

---

.pull-left[
&lt;img src="02_review_files/figure-html/simulation scatter2-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[

What do you notice?

- On **average**, our regression lines match the population line very nicely.

- However, **individual lines** (samples) can really miss the mark.

- Differences between individual samples and the population lead to **uncertainty** for the econometrician.

]

--

**Answer:** **Uncertainty matters.**

`\(\hat{\beta}\)` itself is a random variable—dependent upon the random sample. When we take a sample and run a regression, we don't know if it's a 'good' sample ( `\(\hat{\beta}\)` is close to `\(\beta\)`) or a 'bad sample' (our sample differs greatly from the population).

---
layout: false
# Population *vs.* sample

## Uncertainty

Keeping track of this uncertainty will be a key concept throughout our class.

- Estimating standard errors for our estimates.

- Testing hypotheses.

- Correcting for heteroskedasticity and autocorrelation.

--

But first, let's remind ourselves of how we get these (uncertain) regression estimates.

---
# Linear regression

## The estimator

We can estimate a regression line in .mono[R] (`lm(y ~ x, my_data)`) and .mono[Stata] (`reg y x`). But where do these estimates come from?

A few slides back:

&gt; $$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$
&gt; which gives us the *best-fit* line through our dataset.

But what do we mean by "best-fit line"?

---
layout: false

# Being the "best"

**Question:** What do we mean by *best-fit line*?

**Answers:**

- In general,&lt;sup&gt;†&lt;/sup&gt; *best-fit line* means the line that minimizes the sum of squared errors (SSE):

$$ \text{SSE} = \sum_{i = 1}^{n} e_i^2 $$

&lt;center&gt;where&lt;/center&gt;

$$ e_i = y_i - \hat{y}_i $$

- Ordinary **least squares** (**OLS**) minimizes the sum of the squared errors.
- Based upon a set of (mostly palatable) assumptions, OLS
  - Is unbiased (and consistent)
  - Is the *best*&lt;sup&gt;††&lt;/sup&gt; linear unbiased estimator (BLUE)

.footnote[
[†]: *In general* here means *generally in econometrics*. It's possible to have other definitions (common in machine learning).

[††]: In the case of BLUE, *best* means minimum variance.
]

---
layout: true
# OLS *vs.* other lines/estimators

---

Let's consider the dataset we previously generated.

&lt;img src="02_review_files/figure-html/ols vs lines 1-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

For any line (_i.e._, `\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\)`) that we draw

&lt;img src="02_review_files/figure-html/ols vs lines 2-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

For any line (_i.e._, `\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\)`) that we draw, we can calculate the errors: `\(e_i = y_i - \hat{y}_i\)`

&lt;img src="02_review_files/figure-html/ols vs lines 3-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

For any line (_i.e._, `\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\)`) that we draw, we can calculate the errors: `\(e_i = y_i - \hat{y}_i\)`

&lt;img src="02_review_files/figure-html/ols vs lines 4-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

For any line (_i.e._, `\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\)`) that we draw, we can calculate the errors: `\(e_i = y_i - \hat{y}_i\)`

&lt;img src="02_review_files/figure-html/ols vs lines 5-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

Because SSE squares the errors (_i.e._, `\(\sum e_i^2\)`), big errors are penalized more than small ones.

&lt;img src="02_review_files/figure-html/ols vs lines 6-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

The OLS estimate is the combination of `\(\hat{\beta}_0\)` and `\(\hat{\beta}_1\)` that minimize SSE.

&lt;img src="02_review_files/figure-html/ols vs lines 7-1.svg" style="display: block; margin: auto;" /&gt;

---
layout: true
# OLS

## Formally

---

In simple linear regression, our OLS estimator derives from choosing the `\(\hat{\beta}_0\)` and `\(\hat{\beta}_1\)`, _i.e._,

$$ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SSE} $$

--

But we already know

$$ \text{SSE} = \sum_i e_i^2 $$

Now we use the definitions of `\(e_i\)` and `\(\hat{y}\)` (plus and some algebra)

$$
`\begin{aligned}
  e_i^2 &amp;= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &amp;= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}`
$$

--

**Recall:** Minimizing a multivariate function requires (**1**) first derivatives equal zero (the *first-order conditions*) and (**2**) second-order conditions on concavity.

---

We're getting close. We need to **minimize SSE**, and we've just shown how SSE relates to our sample (our data, _i.e._, `\(x\)` and `\(y\)`) and our estimates (_i.e._, `\(\hat{\beta}_0\)` and `\(\hat{\beta}_1\)`).

$$ \text{SSE} = \sum_i e_i^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) $$

For the first-order conditions of minimization, we now take the first derivates&lt;sup&gt;†&lt;/sup&gt; of SSE with respect to `\(\hat{\beta}_0\)` and `\(\hat{\beta}_1\)`.

$$
`\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} &amp;= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &amp;= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}`
$$

where `\(\overline{x} = \frac{\sum x_i}{n}\)` and `\(\overline{y} = \frac{\sum y_i}{n}\)` give the sample means of `\(x\)` and `\(y\)` (sample size `\(n\)`).

.footnote[
[†]: I'll leave the second-order conditions for you...
]

---

The first-order conditions state that the derivatives are equal to zero, so:

$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0 $$

which implies

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Now for `\(\hat{\beta}_1\)`.

---

Take the derivative of SSE with respect to `\(\hat{\beta}_1\)`

$$
`\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} &amp;= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &amp;= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}`
$$

set it equal to zero (first-order conditions, again)

$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

and substitute in our relationship for `\(\hat{\beta}_0\)`, _i.e._, `\(\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}\)`. Thus,

$$
 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
$$

---

Continuing from the last slide

$$ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

we multiply out

$$ 2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

$$ \implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x} $$

$$ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

---

Done!

We now have (lovely) OLS estimators for the slope

$$ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

and the intercept

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Plus, you know what the *least squares* part of ordinary least squares means. ☺

--

We now turn to the assumptions and (implied) properties of OLS.

---
layout: false
class: inverse, center, middle

# OLS: Assumptions and properties

---
layout: true
# OLS: Assumptions and properties

## Properties

**Question:** What properties might we care about for an estimator?

---

.mono[TODO: Illustrate different possibilities for distributions of estimators.]
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
